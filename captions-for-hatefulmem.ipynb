{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b0b4498",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-06 08:12:23.155297: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2022-05-06 08:12:23.155325: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import clip\n",
    "import os\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as nnf\n",
    "import sys\n",
    "from typing import Tuple, List, Union, Optional\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm, trange\n",
    "from PIL import Image\n",
    "import IPython.display as display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bae788fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "506f2f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pickle as pkl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "261216b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH='/Data_Storage/Rui_Data_Space/textual/hate-speech/domain_splits'\n",
    "IMG_PATH='/Data_Storage/Rui_Data_Space/textual/hate-speech/multimodal-hate/mimc'\n",
    "CLEAN_IMG_PATH='/Data_Storage/Rui_Data_Space/textual/hate-speech/multimodal-hate/mimc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97318b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json(path):\n",
    "    data=json.load(open(path,'r'))\n",
    "    return data\n",
    "\n",
    "def load_pkl(path):\n",
    "    data=pkl.load(open(path,'rb'))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88bdf0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set GPU id\n",
    "CUDA_DEVICE=14\n",
    "torch.cuda.set_device(CUDA_DEVICE)\n",
    "device = torch.device(\"cuda:\"+str(CUDA_DEVICE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "acdd37ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_data='conceptual'#or coco\n",
    "save_path = os.path.join(os.path.dirname('../'), \"pretrained_models\")\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "model_path = os.path.join(save_path, pretrain_data+'_weights.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "53b48e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../pretrained_models/conceptual_weights.pt\n"
     ]
    }
   ],
   "source": [
    "print (model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4f3cf647",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = type(None)\n",
    "V = np.array\n",
    "ARRAY = np.ndarray\n",
    "ARRAYS = Union[Tuple[ARRAY, ...], List[ARRAY]]\n",
    "VS = Union[Tuple[V, ...], List[V]]\n",
    "VN = Union[V, N]\n",
    "VNS = Union[VS, N]\n",
    "T = torch.Tensor\n",
    "TS = Union[Tuple[T, ...], List[T]]\n",
    "TN = Optional[T]\n",
    "TNS = Union[Tuple[TN, ...], List[TN]]\n",
    "TSN = Optional[TS]\n",
    "TA = Union[T, ARRAY]\n",
    "\n",
    "D = torch.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7bf8e6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Model\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def forward(self, x: T) -> T:\n",
    "        return self.model(x)\n",
    "\n",
    "    def __init__(self, sizes: Tuple[int, ...], bias=True, act=nn.Tanh):\n",
    "        super(MLP, self).__init__()\n",
    "        layers = []\n",
    "        for i in range(len(sizes) -1):\n",
    "            layers.append(nn.Linear(sizes[i], sizes[i + 1], bias=bias))\n",
    "            if i < len(sizes) - 2:\n",
    "                layers.append(act())\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "class ClipCaptionModel(nn.Module):\n",
    "\n",
    "    #@functools.lru_cache #FIXME\n",
    "    def get_dummy_token(self, batch_size: int, device: D) -> T:\n",
    "        return torch.zeros(batch_size, self.prefix_length, dtype=torch.int64, device=device)\n",
    "\n",
    "    def forward(self, tokens: T, prefix: T, mask: Optional[T] = None, labels: Optional[T] = None):\n",
    "        embedding_text = self.gpt.transformer.wte(tokens)\n",
    "        prefix_projections = self.clip_project(prefix).view(-1, self.prefix_length, self.gpt_embedding_size)\n",
    "        #print(embedding_text.size()) #torch.Size([5, 67, 768])\n",
    "        #print(prefix_projections.size()) #torch.Size([5, 1, 768])\n",
    "        embedding_cat = torch.cat((prefix_projections, embedding_text), dim=1)\n",
    "        if labels is not None:\n",
    "            dummy_token = self.get_dummy_token(tokens.shape[0], tokens.device)\n",
    "            labels = torch.cat((dummy_token, tokens), dim=1)\n",
    "        out = self.gpt(inputs_embeds=embedding_cat, labels=labels, attention_mask=mask)\n",
    "        return out\n",
    "\n",
    "    def __init__(self, prefix_length: int, prefix_size: int = 512):\n",
    "        super(ClipCaptionModel, self).__init__()\n",
    "        self.prefix_length = prefix_length\n",
    "        self.gpt = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "        self.gpt_embedding_size = self.gpt.transformer.wte.weight.shape[1]\n",
    "        if prefix_length > 10:  # not enough memory\n",
    "            self.clip_project = nn.Linear(prefix_size, self.gpt_embedding_size * prefix_length)\n",
    "        else:\n",
    "            self.clip_project = MLP((prefix_size, (self.gpt_embedding_size * prefix_length) // 2, self.gpt_embedding_size * prefix_length))\n",
    "\n",
    "\n",
    "class ClipCaptionPrefix(ClipCaptionModel):\n",
    "\n",
    "    def parameters(self, recurse: bool = True):\n",
    "        return self.clip_project.parameters()\n",
    "\n",
    "    def train(self, mode: bool = True):\n",
    "        super(ClipCaptionPrefix, self).train(mode)\n",
    "        self.gpt.eval()\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ac66af68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Caption prediction\n",
    "\n",
    "def generate_beam(model, tokenizer, beam_size: int = 5, prompt=None, embed=None,\n",
    "                  entry_length=67, temperature=1., stop_token: str = '.'):\n",
    "\n",
    "    model.eval()\n",
    "    stop_token_index = tokenizer.encode(stop_token)[0]\n",
    "    tokens = None\n",
    "    scores = None\n",
    "    device = next(model.parameters()).device\n",
    "    seq_lengths = torch.ones(beam_size, device=device)\n",
    "    is_stopped = torch.zeros(beam_size, device=device, dtype=torch.bool)\n",
    "    with torch.no_grad():\n",
    "        if embed is not None:\n",
    "            generated = embed\n",
    "        else:\n",
    "            if tokens is None:\n",
    "                tokens = torch.tensor(tokenizer.encode(prompt))\n",
    "                tokens = tokens.unsqueeze(0).to(device)\n",
    "                generated = model.gpt.transformer.wte(tokens)\n",
    "        for i in range(entry_length):\n",
    "            outputs = model.gpt(inputs_embeds=generated)\n",
    "            logits = outputs.logits\n",
    "            logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n",
    "            logits = logits.softmax(-1).log()\n",
    "            if scores is None:\n",
    "                scores, next_tokens = logits.topk(beam_size, -1)\n",
    "                generated = generated.expand(beam_size, *generated.shape[1:])\n",
    "                next_tokens, scores = next_tokens.permute(1, 0), scores.squeeze(0)\n",
    "                if tokens is None:\n",
    "                    tokens = next_tokens\n",
    "                else:\n",
    "                    tokens = tokens.expand(beam_size, *tokens.shape[1:])\n",
    "                    tokens = torch.cat((tokens, next_tokens), dim=1)\n",
    "            else:\n",
    "                logits[is_stopped] = -float(np.inf)\n",
    "                logits[is_stopped, 0] = 0\n",
    "                scores_sum = scores[:, None] + logits\n",
    "                seq_lengths[~is_stopped] += 1\n",
    "                scores_sum_average = scores_sum / seq_lengths[:, None]\n",
    "                scores_sum_average, next_tokens = scores_sum_average.view(-1).topk(beam_size, -1)\n",
    "                next_tokens_source = next_tokens // scores_sum.shape[1]\n",
    "                seq_lengths = seq_lengths[next_tokens_source]\n",
    "                next_tokens = next_tokens % scores_sum.shape[1]\n",
    "                next_tokens = next_tokens.unsqueeze(1)\n",
    "                tokens = tokens[next_tokens_source]\n",
    "                tokens = torch.cat((tokens, next_tokens), dim=1)\n",
    "                generated = generated[next_tokens_source]\n",
    "                scores = scores_sum_average * seq_lengths\n",
    "                is_stopped = is_stopped[next_tokens_source]\n",
    "            next_token_embed = model.gpt.transformer.wte(next_tokens.squeeze()).view(generated.shape[0], 1, -1)\n",
    "            generated = torch.cat((generated, next_token_embed), dim=1)\n",
    "            is_stopped = is_stopped + next_tokens.eq(stop_token_index).squeeze()\n",
    "            if is_stopped.all():\n",
    "                break\n",
    "    scores = scores / seq_lengths\n",
    "    output_list = tokens.cpu().numpy()\n",
    "    output_texts = [tokenizer.decode(output[:int(length)]) for output, length in zip(output_list, seq_lengths)]\n",
    "    order = scores.argsort(descending=True)\n",
    "    output_texts = [output_texts[i] for i in order]\n",
    "    return output_texts\n",
    "\n",
    "\n",
    "def generate2(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        tokens=None,\n",
    "        prompt=None,\n",
    "        embed=None,\n",
    "        entry_count=1,\n",
    "        entry_length=67,  # maximum number of words\n",
    "        top_p=0.8,\n",
    "        temperature=1.,\n",
    "        stop_token: str = '.',\n",
    "):\n",
    "    model.eval()\n",
    "    generated_num = 0\n",
    "    generated_list = []\n",
    "    stop_token_index = tokenizer.encode(stop_token)[0]\n",
    "    filter_value = -float(\"Inf\")\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for entry_idx in trange(entry_count):\n",
    "            if embed is not None:\n",
    "                generated = embed\n",
    "            else:\n",
    "                if tokens is None:\n",
    "                    tokens = torch.tensor(tokenizer.encode(prompt))\n",
    "                    tokens = tokens.unsqueeze(0).to(device)\n",
    "\n",
    "                generated = model.gpt.transformer.wte(tokens)\n",
    "\n",
    "            for i in range(entry_length):\n",
    "\n",
    "                outputs = model.gpt(inputs_embeds=generated)\n",
    "                logits = outputs.logits\n",
    "                logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n",
    "                sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "                cumulative_probs = torch.cumsum(nnf.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "                sorted_indices_to_remove = cumulative_probs > top_p\n",
    "                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[\n",
    "                                                    ..., :-1\n",
    "                                                    ].clone()\n",
    "                sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "                indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "                logits[:, indices_to_remove] = filter_value\n",
    "                next_token = torch.argmax(logits, -1).unsqueeze(0)\n",
    "                next_token_embed = model.gpt.transformer.wte(next_token)\n",
    "                if tokens is None:\n",
    "                    tokens = next_token\n",
    "                else:\n",
    "                    tokens = torch.cat((tokens, next_token), dim=1)\n",
    "                generated = torch.cat((generated, next_token_embed), dim=1)\n",
    "                if stop_token_index == next_token.item():\n",
    "                    break\n",
    "\n",
    "            output_list = list(tokens.squeeze().cpu().numpy())\n",
    "            output_text = tokenizer.decode(output_list)\n",
    "            generated_list.append(output_text)\n",
    "\n",
    "    return generated_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ae0f815e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialization of basic models: clip and gpt-2\n",
    "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device, jit=False)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4940e114",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Load model weights\n",
    "prefix_length = 10\n",
    "model = ClipCaptionModel(prefix_length)\n",
    "model.load_state_dict(torch.load(model_path, map_location=device)) \n",
    "\n",
    "model = model.eval() \n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "720847b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_beam_search = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a12e1fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "Already finished: 0.0\n",
      "Already finished: 2.0\n",
      "Already finished: 4.0\n",
      "Already finished: 6.0\n",
      "Already finished: 8.0\n",
      "Already finished: 10.0\n",
      "Already finished: 12.0\n",
      "Already finished: 14.0\n",
      "Already finished: 16.0\n",
      "Already finished: 18.0\n",
      "Already finished: 20.0\n",
      "Already finished: 22.0\n",
      "Already finished: 24.0\n",
      "Already finished: 26.0\n",
      "Already finished: 28.0\n",
      "Already finished: 30.0\n",
      "Already finished: 32.0\n",
      "Already finished: 34.0\n",
      "Already finished: 36.0\n",
      "Already finished: 38.0\n",
      "Already finished: 40.0\n",
      "Already finished: 42.0\n",
      "Already finished: 44.0\n",
      "Already finished: 46.0\n",
      "Already finished: 48.0\n",
      "Already finished: 50.0\n",
      "Already finished: 52.0\n",
      "Already finished: 54.0\n",
      "Already finished: 56.0\n",
      "Already finished: 58.0\n",
      "Already finished: 60.0\n",
      "Already finished: 62.0\n",
      "Already finished: 64.0\n",
      "Already finished: 66.0\n",
      "Already finished: 68.0\n",
      "Already finished: 70.0\n",
      "Already finished: 72.0\n",
      "Already finished: 74.0\n",
      "Already finished: 76.0\n",
      "Already finished: 78.0\n",
      "Already finished: 80.0\n",
      "Already finished: 82.0\n",
      "Already finished: 84.0\n",
      "Already finished: 86.0\n",
      "Already finished: 88.0\n",
      "Already finished: 90.0\n",
      "Already finished: 92.0\n",
      "Already finished: 94.0\n",
      "Already finished: 96.0\n",
      "Already finished: 98.0\n",
      "1000\n",
      "Already finished: 0.0\n",
      "Already finished: 20.0\n",
      "Already finished: 40.0\n",
      "Already finished: 60.0\n",
      "Already finished: 80.0\n",
      "10995\n"
     ]
    }
   ],
   "source": [
    "#generating image captions over all images\n",
    "files=os.listdir(os.path.join(IMG_PATH,'train'))\n",
    "print(len(files))\n",
    "total={}\n",
    "#random.shuffle(files)\n",
    "for i,f in enumerate(files):\n",
    "    if i%200==0:\n",
    "        print ('Already finished:',i*100.0/len(files))\n",
    "    img_path=os.path.join(IMG_PATH,'train',f)\n",
    "    file_feat=Image.open(img_path)\n",
    "    clip_feat=preprocess(file_feat).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        prefix = clip_model.encode_image(clip_feat).to(device, dtype=torch.float32)\n",
    "        prefix_embed = model.clip_project(prefix).reshape(1, prefix_length, -1)\n",
    "    if use_beam_search:\n",
    "        generated_text_prefix = generate_beam(model, tokenizer, embed=prefix_embed)[0]\n",
    "    else:\n",
    "        generated_text_prefix = generate2(model, tokenizer, embed=prefix_embed)\n",
    "    total[f.split('.')[0]]=generated_text_prefix \n",
    "files=os.listdir(os.path.join(IMG_PATH,'test'))\n",
    "print(len(files))\n",
    "#random.shuffle(files)\n",
    "for i,f in enumerate(files):\n",
    "    if i%200==0:\n",
    "        print ('Already finished:',i*100.0/len(files))\n",
    "    img_path=os.path.join(IMG_PATH,'test',f)\n",
    "    file_feat=Image.open(img_path)\n",
    "    clip_feat=preprocess(file_feat).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        prefix = clip_model.encode_image(clip_feat).to(device, dtype=torch.float32)\n",
    "        prefix_embed = model.clip_project(prefix).reshape(1, prefix_length, -1)\n",
    "    if use_beam_search:\n",
    "        generated_text_prefix = generate_beam(model, tokenizer, embed=prefix_embed)[0]\n",
    "    else:\n",
    "        generated_text_prefix = generate2(model, tokenizer, embed=prefix_embed)\n",
    "    total[f.split('.')[0]]=generated_text_prefix \n",
    "print (len(total))\n",
    "#pkl.dump(total,open('../mimc_conceptual/clean_captions.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8eec5be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl.dump(total,open('../mimc_conceptual/unclean_captions.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b8706f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "Already finished: 0.0\n",
      "Already finished: 20.0\n",
      "Already finished: 40.0\n",
      "Already finished: 60.0\n",
      "Already finished: 80.0\n"
     ]
    }
   ],
   "source": [
    "files=os.listdir(os.path.join(IMG_PATH,'test_clean'))\n",
    "print(len(files))\n",
    "#random.shuffle(files)\n",
    "for i,f in enumerate(files):\n",
    "    if i%200==0:\n",
    "        print ('Already finished:',i*100.0/len(files))\n",
    "    if f.split('.')[0] in total:\n",
    "        continue\n",
    "    img_path=os.path.join(IMG_PATH,'test_clean',f)\n",
    "    file_feat=Image.open(img_path)\n",
    "    clip_feat=preprocess(file_feat).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        prefix = clip_model.encode_image(clip_feat).to(device, dtype=torch.float32)\n",
    "        prefix_embed = model.clip_project(prefix).reshape(1, prefix_length, -1)\n",
    "    if use_beam_search:\n",
    "        generated_text_prefix = generate_beam(model, tokenizer, embed=prefix_embed)[0]\n",
    "    else:\n",
    "        generated_text_prefix = generate2(model, tokenizer, embed=prefix_embed)\n",
    "    total[f.split('.')[0]]=generated_text_prefix \n",
    "print (len(total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa477c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl.dump(total,open('../mimc_conceptual/clean_captions.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "e4573566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already finished: 0.0\n",
      "Already finished: 5.643340857787811\n",
      "Already finished: 11.286681715575622\n",
      "Already finished: 16.93002257336343\n",
      "Already finished: 22.573363431151243\n",
      "Already finished: 28.216704288939052\n",
      "Already finished: 33.86004514672686\n",
      "Already finished: 39.503386004514674\n",
      "Already finished: 45.146726862302486\n",
      "Already finished: 50.79006772009029\n",
      "Already finished: 56.433408577878104\n",
      "Already finished: 62.07674943566592\n",
      "Already finished: 67.72009029345372\n",
      "Already finished: 73.36343115124153\n",
      "Already finished: 79.00677200902935\n",
      "Already finished: 84.65011286681715\n",
      "Already finished: 90.29345372460497\n",
      "Already finished: 95.93679458239278\n"
     ]
    }
   ],
   "source": [
    "#generating image captions over all images\n",
    "files=os.listdir(CLEAN_IMG_PATH)\n",
    "total={}\n",
    "#random.shuffle(files)\n",
    "for i,f in enumerate(files):\n",
    "    if i%200==0:\n",
    "        print ('Already finished:',i*100.0/len(files))\n",
    "    img_path=os.path.join(CLEAN_IMG_PATH,f)\n",
    "    file_feat=Image.open(img_path)\n",
    "    clip_feat=preprocess(file_feat).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        prefix = clip_model.encode_image(clip_feat).to(device, dtype=torch.float32)\n",
    "        prefix_embed = model.clip_project(prefix).reshape(1, prefix_length, -1)\n",
    "    if use_beam_search:\n",
    "        generated_text_prefix = generate_beam(model, tokenizer, embed=prefix_embed)[0]\n",
    "    else:\n",
    "        generated_text_prefix = generate2(model, tokenizer, embed=prefix_embed)\n",
    "    total[f.split('.')[0]]=generated_text_prefix \n",
    "pkl.dump(total,open('../harm/clean_captions.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133164ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "a43d961f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.dpi']=120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecf97db",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "clean_total=load_pkl('../mem_conceptual/clean_captions.pkl')\n",
    "unclean_total=load_pkl('../mem_conceptual/unclean_captions.pkl')\n",
    "#unclean_total=load_pkl('../mem_coco/unclean_captions.pkl')\n",
    "print (len(clean_total),len(unclean_total))\n",
    "name_list=list(clean_total.keys())\n",
    "random.shuffle(name_list)\n",
    "for k,name in enumerate(name_list):\n",
    "    if k>5:\n",
    "        break\n",
    "    img_path=os.path.join(IMG_PATH,name+'.png')\n",
    "    file_feat=Image.open(img_path)\n",
    "    img = file_feat.convert('RGB')\n",
    "    img = np.asarray(img, dtype=np.float32).squeeze()\n",
    "    fig = plt.figure()\n",
    "    ax=fig.add_subplot(1,1,1)\n",
    "    ax.set_title('Clean: \\n'+clean_total[name]+'\\nUnclean:\\n'+unclean_total[name],\n",
    "                 fontsize=12)\n",
    "    ax.imshow(img.astype(np.uint8)) \n",
    "    \n",
    "    \"\"\"img_path=os.path.join(IMG_PATH,name+'.png')\n",
    "    file_feat=Image.open(img_path)\n",
    "    img = file_feat.convert('RGB')\n",
    "    img = np.asarray(img, dtype=np.float32).squeeze()\n",
    "    fig = plt.figure()\n",
    "    ax=fig.add_subplot(1,1,1)\n",
    "    ax.set_title('Unclean: \\n\\t'+unclean_total[name],\n",
    "                 fontsize=12)\n",
    "    ax.imshow(img.astype(np.uint8)) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07497876",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbeda2fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee290a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0767c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c414dd61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37b71cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec4f7e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55283454",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
