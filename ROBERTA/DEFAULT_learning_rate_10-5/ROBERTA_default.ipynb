{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0JjgAQPXLxu-",
        "outputId": "eefc630f-98d0-4634-9bd6-466c3a624603"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sun Nov 19 10:47:38 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   55C    P8    11W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_b1iJs0MWmi",
        "outputId": "ed1e3cc8-9745-4a27-f8f3-a45dcb6e2bb2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'prompthate'...\n",
            "warning: redirecting to https://gitlab.com/bottle_shop/safe/prompthate.git/\n",
            "remote: Enumerating objects: 95, done.\u001b[K\n",
            "remote: Total 95 (delta 0), reused 0 (delta 0), pack-reused 95\u001b[K\n",
            "Receiving objects: 100% (95/95), 3.74 MiB | 22.03 MiB/s, done.\n",
            "Resolving deltas: 100% (30/30), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://gitlab.com/bottle_shop/safe/prompthate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JhZJomT4Mcjp",
        "outputId": "528203f0-61a7-4178-eb91-ed8b0bee3edf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/prompthate/PromptHate-Code\n"
          ]
        }
      ],
      "source": [
        "%cd /content/prompthate/PromptHate-Code/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DbY2eC-fMf0O",
        "outputId": "26fb3150-1110-4307-e8d6-138e5a0187f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "alternative_roberta_baseline.py  dataset.py  processor.py         roberta_dataset.py\n",
            "baseline.py                      \u001b[0m\u001b[01;34mharm\u001b[0m/       \u001b[01;34m__pycache__\u001b[0m/         run.sh\n",
            "classifier.py                    main.py     rela_encoder.py      train.py\n",
            "config.py                        \u001b[01;34mmem\u001b[0m/        roberta_baseline.py  utils.py\n"
          ]
        }
      ],
      "source": [
        "%ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_npFU55MiYl",
        "outputId": "2f49e0b0-2493-4bec-b858-6d3f2083c1c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab.json: 100% 899k/899k [00:00<00:00, 8.48MB/s]\n",
            "merges.txt: 100% 456k/456k [00:00<00:00, 6.14MB/s]\n",
            "tokenizer.json: 100% 1.36M/1.36M [00:00<00:00, 24.3MB/s]\n",
            "config.json: 100% 482/482 [00:00<00:00, 3.38MB/s]\n",
            "100% 1/1 [00:05<00:00,  5.76s/it]\n",
            "100% 1/1 [00:00<00:00,  4.77it/s]\n",
            "model.safetensors: 100% 1.42G/1.42G [00:12<00:00, 111MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "100% 1/1 [00:00<00:00,  5.08it/s]\n",
            "100% 1/1 [00:00<00:00,  5.06it/s]\n",
            "100% 1/1 [00:00<00:00,  4.97it/s]\n",
            "100% 1/1 [00:00<00:00,  5.00it/s]\n",
            "100% 1/1 [00:00<00:00,  5.11it/s]\n",
            "100% 1/1 [00:00<00:00,  4.80it/s]\n",
            "100% 1/1 [00:00<00:00,  5.04it/s]\n",
            "100% 1/1 [00:00<00:00,  4.89it/s]\n",
            "100% 1/1 [00:00<00:00,  5.15it/s]\n",
            "100% 1/1 [00:00<00:00,  5.13it/s]\n",
            "100% 1/1 [00:00<00:00,  5.09it/s]\n",
            "100% 1/1 [00:00<00:00,  5.43it/s]\n",
            "100% 1/1 [00:00<00:00,  4.30it/s]\n",
            "100% 1/1 [00:00<00:00,  4.80it/s]\n",
            "100% 1/1 [00:00<00:00,  4.44it/s]\n",
            "100% 1/1 [00:00<00:00,  2.31it/s]\n",
            "100% 1/1 [00:00<00:00,  5.25it/s]\n",
            "100% 1/1 [00:00<00:00,  2.64it/s]\n",
            "100% 1/1 [00:00<00:00,  5.05it/s]\n",
            "100% 1/1 [00:00<00:00,  5.25it/s]\n",
            "100% 1/1 [00:00<00:00,  5.37it/s]\n",
            "100% 1/1 [00:00<00:00,  5.00it/s]\n",
            "100% 1/1 [00:00<00:00,  2.55it/s]\n",
            "100% 1/1 [00:00<00:00,  4.62it/s]\n",
            "100% 1/1 [00:00<00:00,  5.44it/s]\n",
            "100% 1/1 [00:00<00:00,  2.45it/s]\n",
            "100% 1/1 [00:00<00:00,  5.13it/s]\n",
            "100% 1/1 [00:00<00:00,  4.74it/s]\n",
            "100% 1/1 [00:00<00:00,  5.27it/s]\n",
            "100% 1/1 [00:00<00:00,  3.23it/s]\n",
            "100% 1/1 [00:00<00:00,  5.32it/s]\n",
            "100% 1/1 [00:00<00:00,  4.98it/s]\n"
          ]
        }
      ],
      "source": [
        "!bash run.sh"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}